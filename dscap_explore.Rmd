DS Capstone NLP Exploratory Analysis
=============================

# Introduction  

This report endeavors to explore and understand a three large text files. The text has been collected from three sources: blogs, news and Twitter. The data is from a corpus called [HC Corpora](www.corpora.heliohost.org). See the [readme file](http://www.corpora.heliohost.org/aboutcorpus.html) for details on the corpora available.  

The text files will ultimately be used to test and train a model that will predict the next word in an unfinished sentence, along the lines of the app [SwiftKey](https://itunes.apple.com/app/swiftkey-keyboard/id911813648?ls=1&mt=8), which descibes itself as "a smart keyboard that learns from you, replacing your deviceâ€™s built-in keyboard with one that adapts to the way you type. The app learns your writing style to give you super-accurate autocorrect and intelligent next-word prediction, reducing keystrokes and getting smarter over time." 

```{r setOpts, warning=FALSE, message=FALSE, echo=FALSE}
library("knitr")
library("tm")
library("RWeka")
library("plyr")
library("ggplot2")
library("gridExtra")
library("SnowballC")
opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, fig.height=4, fig.width=4, echo=FALSE)
```
**Helpful Terminology**  
Corpus: a series of text files analyzed together as a set  
Token: a piece of language; a word  
Tokenize: split text into individual pieces/words  

```{r loadSamples}
news <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.news.sample3.txt", stringsAsFactors=FALSE)
twitter <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.twitter.sample3.txt", stringsAsFactors=FALSE)
blog <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.blog.sample3.txt", stringsAsFactors=FALSE)

#news <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.news.sample3.txt", stringsAsFactors=FALSE)
#twitter <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.twitter.sample3.txt", stringsAsFactors=FALSE)
#blog <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.blog.sample3.txt", stringsAsFactors=FALSE)

allSamples <- rbind(news, twitter, blog)
#write.csv(allSamples,"allSamples.csv", row.names=FALSE)
```

# Exploratory Analysis  

Our exploratory analysis will include:  

- looking at descriptive statistics regarding the size and shape of the corpus;  
- cleaning the corpus;  
- tokenizing the corpus;  
- describing the features of the tokenized corpus; and  
- estimating the size of corpus needed to satisfy our task.   

### Describing the data set  
The three files in their entirety amount to more than 500 Mb and take several minutes just to load into the R workspace, so each file was sampled at 3% to produce files small enough to work with. 

The Twitter data set is by far the longest, with more than twice as many lines as the news and blog data sets, though the blog data set leads in the total number of tokens and the number of unique tokens.  

```{r tokenTable}
newsTT <- WordTokenizer(unlist(news))
twitterTT <- WordTokenizer(unlist(twitter))
blogTT <- WordTokenizer(unlist(blog))

avgChar <- function (x) round(sum(nchar(unlist(x)))/dim(x)[1],0)
avgTokens  <- function (x,y) round(length(y)/dim(x)[1],0)

wordTable <- data.frame(
    Corpus=c("News 3% Sample", "Twitter 3% Sample", "Blog 3% Sample"), 
    Lines=c(dim(news)[1], dim(twitter)[1], dim(blog)[1]),
    Avg.Char.Line=c(avgChar(news), avgChar(twitter), avgChar(blog)),
    Avg.Tokens.Line=c(avgTokens(news, newsTT), avgTokens(twitter, twitterTT), avgTokens(blog, blogTT)),
    Total.Tokens=c(length(newsTT), length(twitterTT), length(blogTT)), 
    Unique.Tokens=c(length(unique(newsTT)), length(unique(twitterTT)), length(unique(blogTT)))
    )

kable(wordTable, row.names=FALSE)
``` 
  
**Sample lines**  
```{r sampleLines}
paste("NEWS:  ", news[110,])
paste("TWITTER:  ", twitter[110,])
paste("BLOG:  ", blog[110,])
```

### Cleaning the data  set

**Punctuation and numbers**  
For simplicity, I've decided to handle puncutation and numbers by removing them completely. In general, numbers and punctuation don't help predict the next word, e.g. in the phrase "At the park there were 5 ducks," the word ducks is not associated or dependent on the number 5 at all. The same can be said for most punctuation (with aprostrophes posing the biggest problem). 

**Capitalization**  
All words will be transformed into lowercase. This poses some potential issues were tokens that are not the same will be treated the same - such as the name "Bill" vs. "bill." 

**Stopwords**  
I have intentionally left in the English stopwords - a, an, and, the, but, to - because they play an important role in linking text. If the goal was to analyze sentiment or determine subject, we could leave them out, but for predicting words, you need "to" in "I went to the store" for it to make sense.  

**Profanity**  
?????  
I'm not sure that I mind profanity. Most swear words are adjectives, which wouldn't cause too many problems to remove.     

**Typos, garbage and foreign languages**  
??????  

**Cleaning**  
To clean the data, I will reload in the data as a corpus in order to use the `tm` package.  The order of cleaning is:  

- Remove numbers  
- Remove punctuation  
- Strip whitespace  
- Transform to lowercase  

```{r exploreCorpus, echo=TRUE}
setwd("/Users/kwalker/git_projects/DS_Capstone/")
#setwd("/Users/kaylinwalker/R/DS_Capstone")
corpus <- Corpus(DirSource("data"), readerControl=list(language="en"))

corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))

#writeCorpus(corpus, path=".", filenames=paste(seq_along(corpus)))
```

### Tokenizing the data set  

Using the `RWeka` package in conjunction with the `tm` package, I have created three term document matrices - one for unigrams, one for bigrams, and one for trigrams.  


```{r tokenize, echo=TRUE}
options(mc.cores=1)
unigrams <- function(x) NGramTokenizer(x, Weka_control(min=0, max=1))
bigrams <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigrams <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
quadgrams <- function(x) NGramTokenizer(x, Weka_control(min=4, max=4))

uniCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = unigrams, wordLengths=c(1,Inf)))
biCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = bigrams))
triCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = trigrams))
quadCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = quadgrams))
```

### Describing the tokenized data set  
*Some words are more frequent than others - what are the distributions of word frequencies?*  

```{r freqWords, echo=TRUE}
freqFunc <- function(corpus) {
     terms <- findFreqTerms(corpus, lowfreq = 5)
     table <- rowSums(as.matrix(corpus[terms,]))
     table <- data.frame(word=names(table), freq=table)
     table <- table[order(-table$freq), ]
     table$rank <- 1:length(table[,1])
     return(table)
}
freqUni <- freqFunc(uniCorpus)
freqBi <- freqFunc(biCorpus)
freqTri <- freqFunc(triCorpus)
freqQuad <- freqFunc(quadCorpus)

gramTable <- cbind(freqUni[1:10, c(3,1,2) ], freqBi[1:10, 1:2],freqTri[1:10, 1:2 ])
colnames(gramTable) <- c("rank", "uni-gram", "f", "bi-gram", "f", "tri-gram", "f")

kable(gramTable, row.names=FALSE)

hist(log10(freqUni$freq),  main="Log10 Uni-gram Frequency")
hist(log10(freqBi$freq), main="Log10 Bi-gram Frequency")
hist(log10(freqTri$freq), main="Log10 Tri-gram Frequency")

```

### Zipf's Law  

Via [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law): "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table." Zipf's Law holds for our corpus.  

```{r zipfPlot, echo=TRUE}
plot(x=log10(freqUni$freq), y=log10(freqUni$rank), type="l", main="Zipf's Law")

# find how many terms are only used once
lonelyRiders <- function(x) {
    singles <- rowSums(as.matrix(x))
    singles <- singles[order(singles)]
    singles <- subset(singles, singles == 1)
    singles <- length(singles)
    singleRate <- singles/dim(x)[1]
    return(singles)
}
lonelyUni <- lonelyRiders(uniCorpus)
lonelyBi <- lonelyRiders(biCorpus)
lonelyTri <- lonelyRiders(triCorpus)
lonelyQuad <- lonelyRiders(quadCorpus)
```



### Compressing the data set  

```{r}
# How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?

# Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

freqUni$Sum <- cumsum(freqUni$freq)
uni50 <- subset(freqUni, freqUni$Sum >  sum(freqUni$freq)*0.5) #266
uni90 <- subset(freqUni, freqUni$Sum >  sum(freqUni$freq)*0.9) 

compressFunc <- function(fTable, fCorpus) {
     fTable$Sum <- cumsum(fTable$freq)
     majority <- subset(fTable, fTable$Sum >  sum(fTable$freq)*0.9) 
     cutoff <- majority[1,3]
     dictionary <- fTable[fTable$rank < cutoff, 1]
     dictionary <- as.character(unlist(dictionary))
     compress <- rowSums(as.matrix(fCorpus[dictionary,]))
     compress <- data.frame(word=dictionary, freq=compress)
}

uniCompress <- compressFunc(freqUni, uniCorpus)
biCompress <- compressFunc(freqBi, biCorpus)
triCompress <- compressFunc(freqTri, triCorpus)
quadCompress <- compressFunc(freqQuad, quadCorpus)

#allCompress <- compressFunc(freqAll, allCorpus)
#write.csv(allCompress, "allCompress.csv", row.names=FALSE)
#write.csv(uniCompress, "uniCompress.csv", row.names=FALSE)
#write.csv(biCompress, "biCompress.csv", row.names=FALSE)
#write.csv(triCompress, "triCompress.csv", row.names=FALSE)
#write.csv(quadCompress, "quadCompress.csv", row.names=FALSE)

```
To compute this, I calculated a cumulative sum of the token frequencies and stopped at the row that passed 50%/90% of total tokens.  

To cover 50% of all word instances, a dictionary of only **`r uni50[1,3]`** words are needed. To cover 90% of all word instances, a dictionary of **`r uni90[1,3]`** words are needed.

# Next Steps  

Next steps include:  
- Creating a corpus each for bi-, tri- and 4 grams;  
- compressing each corpora to cover 90% of tokens;  
- developing a model to match the most likely bi-, tri- or 4 gram given an input string (using a Markov Chain);  
- train the model and compare to test set;  
- optimzing for running speed;  
- implement a Shiny app.  


