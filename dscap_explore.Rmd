DS Capstone NLP Exploratory Analysis
=============================
kwalker `r date()`

# Introduction  

This report endeavors to explore and understand a three large text files. The text has been collected from three sources: blogs, news and Twitter. The data is from a corpus called [HC Corpora](www.corpora.heliohost.org). See the [readme file](http://www.corpora.heliohost.org/aboutcorpus.html) for details on the corpora.  

The text files will ultimately be used to test and train a model that will predict the next word in an unfinished sentence, along the lines of the app [SwiftKey](https://itunes.apple.com/app/swiftkey-keyboard/id911813648?ls=1&mt=8), which descibes itself as "a smart keyboard that learns from you, replacing your deviceâ€™s built-in keyboard with one that adapts to the way you type. The app learns your writing style to give you super-accurate autocorrect and intelligent next-word prediction, reducing keystrokes and getting smarter over time." 

```{r setOpts, warning=FALSE, message=FALSE, echo=FALSE}
library("knitr")
library("tm")
library("RWeka")
library("plyr")
library("ggplot2")
library("gridExtra")
library("SnowballC")
opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, fig.height=4, fig.width=4, echo=FALSE)
```
**Helpful Terminology**  
Corpus: a series of text files analyzed together as a set  
Token: a piece of language; a word  
Tokenize: split text into individual pieces/words  

```{r loadSamples}
news <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.news.train.03.txt", stringsAsFactors=FALSE)
twitter <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.twitter.train.03.txt", stringsAsFactors=FALSE)
blog <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.blog.train.03.txt", stringsAsFactors=FALSE)
allSamples <- rbind(news, twitter, blog)
#write.table(allSamples, "en_US.all.txt")
```

# Exploratory Analysis  

Our exploratory analysis will include:  

- looking at descriptive statistics regarding the size and shape of the corpus;  
- cleaning the corpus;  
- tokenizing the corpus;  
- describing the features of the tokenized corpus; and  
- estimating the size of corpus needed to satisfy our task.   

### Describing the data set  
The three files in their entirety amount to more than 500 Mb and take several minutes just to load into the R workspace, so each file was sampled at 3% to produce files small enough to work with (with about 0.25% of each file was reserved for testing later on).   

The Twitter data set is by far the longest, with more than twice as many lines as the news and blog data sets, though the blog data set leads in the total number of tokens and the number of unique tokens.  

```{r tokenTable}
newsTT <- WordTokenizer(unlist(news))
twitterTT <- WordTokenizer(unlist(twitter))
blogTT <- WordTokenizer(unlist(blog))

avgChar <- function (x) round(sum(nchar(unlist(x)))/dim(x)[1],0)
avgTokens  <- function (x,y) round(length(y)/dim(x)[1],0)

wordTable <- data.frame(
    Corpus=c("News 3% Sample", "Twitter 3% Sample", "Blog 3% Sample"), 
    Lines=c(dim(news)[1], dim(twitter)[1], dim(blog)[1]),
    Avg.Char.Line=c(avgChar(news), avgChar(twitter), avgChar(blog)),
    Avg.Tokens.Line=c(avgTokens(news, newsTT), avgTokens(twitter, twitterTT), avgTokens(blog, blogTT)),
    Total.Tokens=c(length(newsTT), length(twitterTT), length(blogTT)), 
    Unique.Tokens=c(length(unique(newsTT)), length(unique(twitterTT)), length(unique(blogTT)))
    )

kable(wordTable, row.names=FALSE)
``` 
  
**Sample lines**  
```{r sampleLines}
paste("NEWS:  ", news[110,])
paste("TWITTER:  ", twitter[110,])
paste("BLOG:  ", blog[110,])
```

### Cleaning the data  set

**Punctuation and numbers**  
For simplicity, I've decided to handle puncutation and numbers by removing them completely. In general, numbers and punctuation don't help predict the next word, e.g. in the phrase "At the park there were 5 ducks," the word ducks is not associated or dependent on the number 5 at all. The same can be said for most punctuation (with aprostrophes posing the biggest problem). 

**Capitalization**  
All words will be transformed into lowercase. This poses some potential issues were tokens that are not the same will be treated the same - such as the name "Bill" vs. "bill." 

**Stopwords**  
I have intentionally left in the English stopwords - a, an, and, the, but, to - because they play an important role in linking text. If the goal was to analyze sentiment or determine subject, we could leave them out, but for predicting words, you need "to" in "I went to the store" for it to make sense.  

**Profanity**  
I don't mind profanity. If the explicit word appears enough times to be the most probable next word, then it should be kept in the dataset.      

**Cleaning**  
To clean the data, I will reload in the data as a corpus in order to use the `tm` package.  The order of cleaning is:  

- Remove numbers  
- Remove punctuation  
- Strip whitespace  
- Transform to lowercase  

```{r exploreCorpus, echo=TRUE}
setwd("/Users/kwalker/git_projects/DS_Capstone/data")
corpus <- Corpus(DirSource("all"), readerControl=list(language="en")) # read in a file that contains all three data sets
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))

```

### Tokenizing the data set  
Tokenizing the data will split the text files (corpus) up into bite size pieces - words. We can choose how big we want the bite sizes to be: unigrams are one word; bigrams are two words; trigrams are 3 words, and so on. Using the `RWeka` package in conjunction with the `tm` package, I have created four term document matrices - one for unigrams, one for bigrams, one for trigrams and one for quadgrams.  


```{r tokenize, echo=TRUE}
options(mc.cores=1)
unigrams <- function(x) NGramTokenizer(x, Weka_control(min=0, max=1))
bigrams <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigrams <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
quadgrams <- function(x) NGramTokenizer(x, Weka_control(min=4, max=4))

uniCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = unigrams, wordLengths=c(1,Inf)))
biCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = bigrams))
triCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = trigrams))
quadCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = quadgrams))
```

### Describing the tokenized data set  
Some words are more frequent than others - what are the distributions of word frequencies? It's clear that the majority of the corpus is made up of relatively few highly repeated words, accompanied by a huge percentage of "rare" or "sparse" terms that only appear a few times.   

```{r freqWords, echo=TRUE}
freqFunc <- function(corpus) {
     terms <- findFreqTerms(corpus, lowfreq = 5)
     table <- rowSums(as.matrix(corpus[terms,]))
     table <- data.frame(word=names(table), freq=table)
     table <- table[order(-table$freq), ]
     table$rank <- 1:length(table[,1])
     return(table)
}
freqUni <- freqFunc(uniCorpus)
freqBi <- freqFunc(biCorpus)
freqTri <- freqFunc(triCorpus)
freqQuad <- freqFunc(quadCorpus)

gramTable <- cbind(freqUni[1:10, c(3,1,2) ], freqBi[1:10, 1:2],freqTri[1:10, 1:2 ])
colnames(gramTable) <- c("rank", "uni-gram", "f", "bi-gram", "f", "tri-gram", "f")

kable(gramTable, row.names=FALSE)

hist(log10(freqUni$freq),  main="Log10 Uni-gram Frequency")
hist(log10(freqBi$freq), main="Log10 Bi-gram Frequency")
hist(log10(freqTri$freq), main="Log10 Tri-gram Frequency")

```

### Zipf's Law  

Via [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law): "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table." Zipf's Law holds for our corpus.  

```{r zipfPlot, echo=TRUE}
plot(x=log10(freqUni$freq), y=log10(freqUni$rank), type="l", main="Zipf's Law")
```

```{r smoothing}
# find how many terms are only used once
lonelyRiders <- function(x) {
    singles <- rowSums(as.matrix(x))
    singles <- singles[order(singles)]
    singles <- subset(singles, singles == 1)
    singles <- length(singles)
    singleRate <- singles/dim(x)[1]
    return(singleRate)
}
lonelyUni <- lonelyRiders(uniCorpus)
lonelyBi <- lonelyRiders(biCorpus)
lonelyTri <- lonelyRiders(triCorpus)
lonelyQuad <- lonelyRiders(quadCorpus)
```



### Compressing the data set  

```{r}
freqUni$Sum <- cumsum(freqUni$freq)
uni50 <- subset(freqUni, freqUni$Sum >  sum(freqUni$freq)*0.5) #266
uni90 <- subset(freqUni, freqUni$Sum >  sum(freqUni$freq)*0.9) 

compressFunc <- function(fTable, fCorpus) {
     fTable$Sum <- cumsum(fTable$freq)
     majority <- subset(fTable, fTable$Sum >  sum(fTable$freq)*0.9) 
     cutoff <- majority[1,3]
     dictionary <- fTable[fTable$rank < cutoff, 1]
     dictionary <- as.character(unlist(dictionary))
     compress <- rowSums(as.matrix(fCorpus[dictionary,]))
     compress <- data.frame(word=dictionary, freq=compress)
}

uniCompress <- compressFunc(freqUni, uniCorpus)
biCompress <- compressFunc(freqBi, biCorpus)
triCompress <- compressFunc(freqTri, triCorpus)
quadCompress <- compressFunc(freqQuad, quadCorpus)
```
An interesting question was asked, which will become important as size becomes an issue: How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
To compute this, I calculated a cumulative sum of the token frequencies and stopped at the row that passed 50%/90% of total tokens.  

To cover 50% of all word instances, a dictionary of only **`r uni50[1,3]`** words are needed. To cover 90% of all word instances, a dictionary of **`r uni90[1,3]`** words are needed.

# Next Steps  

Next steps include:  
- Creating a corpus each for bi-, tri- and 4 grams;  
- compressing each corpora to cover 90% of tokens;  
- developing a model to match the most likely bi-, tri- or 4 gram given an input string (using a Markov Chain);  
- train the model and compare to test set;  
- optimzing for running speed;  
- implement a Shiny app.  


