Data Science Capstone NLP Exploratory Analysis
=============================

# Introduction  
--- mention sampling reasoning (5%)

```{r setOpts, warning=FALSE, message=FALSE, echo=FALSE}
library("knitr")
library("tm")
library("RWeka")
library("plyr")
library("ggplot2")
library("gridExtra")
library("SnowballC")
opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, fig.height=5, fig.width=5, echo=FALSE)
```

## Load in the sample data  
```{r loadSamples}
news <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.news.sample.txt", stringsAsFactors=FALSE)
twitter <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.twitter.sample.txt", stringsAsFactors=FALSE)
blog <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.blog.sample.txt", stringsAsFactors=FALSE)
```

## Explore the data  

```{r tokenTable}
newsTT <- WordTokenizer(unlist(news))
twitterTT <- WordTokenizer(unlist(twitter))
blogTT <- WordTokenizer(unlist(blog))

wordTable <- data.frame(
    Corpus=c("News 5% Sample", "Twitter 5% Sample", "Blog 5% Sample"), 
    Total.Tokens=c(length(newsTT), length(twitterTT), length(blogTT)), 
    Unique.Tokens=c(length(unique(newsTT)), length(unique(twitterTT)), length(unique(blogTT)))
    )

kable(wordTable, row.names=FALSE)
``` 

## Clean the data  

```{r exploreCorpus}
# load in the data
setwd("/Users/kwalker/git_projects/DS_Capstone/")
corpus <- Corpus(DirSource("/Users/kwalker/git_Projects/DS_Capstone/testdata"), readerControl=list(language="en"))

# clean the corpus
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, stemDocument)
```

## Tokenization  

```{r tokenize}
# format to tdm and tokenize
options(mc.cores=1)
unigrams <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
bigrams <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigrams <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

uniCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = unigrams))
biCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = bigrams))
triCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = trigrams))

```

## Word frequency  

```{r freqWords}
# Some words are more frequent than others - what are the distributions of word frequencies? 

freqTerms <- findFreqTerms(uniCorpus, lowfreq = 5)
freqTable <- rowSums(as.matrix(uniCorpus[freqTerms,]))
freqTable <- data.frame(word=names(freqTable), freq=freqTable)
freqTable <- freqTable[order(-freqTable$freq), ]
freqTable$rank <- 1:length(freqTable[,1])
kable(freqTable[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable$freq))

```

### Zipf's Law  

Via [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law): "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table."

Zipf's Law holds for all three of our corpora.  

```{r zipfPlot}
ggplot(freqTable, aes(log10(freq), log10(rank))) + geom_line(color="red") + labs(title="Zipf's Law") + theme(panel.background=element_blank())
```

## Bi- and tri-gram frequency  

```{r ngramFreq}
# What are the frequencies of 2-grams and 3-grams in the dataset? 
freqTerms2 <- findFreqTerms(biCorpus, lowfreq = 5)
freqTable2 <- rowSums(as.matrix(biCorpus[freqTerms2,]))
freqTable2 <- data.frame(word=names(freqTable2), freq=freqTable2)
freqTable2 <- freqTable2[order(-freqTable2$freq), ]
freqTable2$rank <- 1:length(freqTable2[,1])
kable(freqTable2[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable2$freq))

freqTerms3 <- findFreqTerms(triCorpus, lowfreq = 5)
freqTable3 <- rowSums(as.matrix(triCorpus[freqTerms3,]))
freqTable3 <- data.frame(word=names(freqTable3), freq=freqTable3)
freqTable3 <- freqTable3[order(-freqTable3$freq), ]
freqTable3$rank <- 1:length(freqTable3[,1])
kable(freqTable3[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable3$freq))

```

## Language coverage  

```{r}
# How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 
allWordInstances <- sum(freqTable$freq)
halfWI <- allWordInstances*0.5
ninetyWI <- allWordInstances*0.9

freqTable$Sum <- cumsum(freqTable$freq)

fifty <- subset(freqTable, freqTable$Sum > halfWI)
fifty <- fifty[1,3]

ninety <- subset(freqTable, freqTable$Sum > ninetyWI)
ninety <- ninety[1,3]
```

To cover 50% of all word instances, a dictionary of only **`r fifty`** words are needed. To cover 90% of all word instances, a dictionary of **`r ninety`** words are needed.


```{r}
# How do you evaluate how many of the words come from foreign languages? 

```

```{r}
# Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

```


```{r cleanCorpus}
# Remove unwanted characters (HTML tags, etc.)

# Word boundaires (white space, punctuation [consider Ph.D., isn't, e-mail])
#test <- twitter[1:10,]
#test <- paste(test, collapse=" ")
#test <- strsplit(test, "\\?+ |\\!+ |\\.+ ")

# Stemming (optional, most common is Porter Stemmer)

# Stopword removal (the, a, of, for, in; SMART is a common stopword list) - is this helpful for this ???

```

```{r trainPredictSimple}
# combine corpora
# separate into train and test sets
# get frequency distribution of two word ngrams


m <- Corpus(VectorSource(blog))
```
