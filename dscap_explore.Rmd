Data Science Capstone NLP Exploratory Analysis
=============================

# Introduction  
--- mention sampling reasoning (5%)

```{r setOpts, warning=FALSE, message=FALSE, echo=FALSE}
library("knitr")
library("tm")
library("RWeka")
library("plyr")
library("ggplot2")
library("gridExtra")
library("SnowballC")
opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, fig.height=5, fig.width=5, echo=FALSE)
```

## Load in the data 
The three files in their entirety amount to more than 500 Mb and take several minutes just to load into the R workspace, so each file was sampled at 5% to produce files small enough to work with. 

```{r loadSamples}
#news <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.news.sample.txt", stringsAsFactors=FALSE)
#twitter <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.twitter.sample.txt", stringsAsFactors=FALSE)
#blog <- read.table("/Users/kwalker/git_projects/DS_Capstone/data/en_US.blog.sample.txt", stringsAsFactors=FALSE)

news <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.news.sample.txt", stringsAsFactors=FALSE)
twitter <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.twitter.sample.txt", stringsAsFactors=FALSE)
blog <- read.table("/Users/kaylinwalker/R/DS_Capstone/data/en_US.blog.sample.txt", stringsAsFactors=FALSE)
```

## Explore the data  

The Twitter data set is by far the longest, with more than twice as many lines as the news and blog data sets, though the blog data set leads in the total number of tokens and the number of unique tokens.  

```{r tokenTable}
newsTT <- WordTokenizer(unlist(news))
twitterTT <- WordTokenizer(unlist(twitter))
blogTT <- WordTokenizer(unlist(blog))

avgChar <- function (x) round(sum(nchar(unlist(x)))/dim(x)[1],0)
avgTokens  <- function (x,y) round(length(y)/dim(x)[1],0)

wordTable <- data.frame(
    Corpus=c("News 5% Sample", "Twitter 5% Sample", "Blog 5% Sample"), 
    Lines=c(dim(news)[1], dim(twitter)[1], dim(blog)[1]),
    Avg.Char.Line=c(avgChar(news), avgChar(twitter), avgChar(blog)),
    Avg.Tokens.Line=c(avgTokens(news, newsTT), avgTokens(twitter, twitterTT), avgTokens(blog, blogTT)),
    Total.Tokens=c(length(newsTT), length(twitterTT), length(blogTT)), 
    Unique.Tokens=c(length(unique(newsTT)), length(unique(twitterTT)), length(unique(blogTT)))
    )

kable(wordTable, row.names=FALSE)
``` 

## Clean the data  
To clean the data, we will reload in the data as a corpus in order to use the `tm` package.  

The order of cleaning is:  
- Remove numbers  
- Remove punctuation  
- Strip whitespace  
- Transform to lowercase  

```{r exploreCorpus, echo=TRUE}
#setwd("/Users/kwalker/git_projects/DS_Capstone/")
setwd("/Users/kaylinwalker/R")
corpus <- Corpus(DirSource("DS_Capstone/testdata"), readerControl=list(language="en"))

corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
```

## Tokenization  

Using the `RWeka` package in conjunction with the `tm` package, I have created three term document matrices - one for unigrams, one for bigrams, and one for trigrams.  


```{r tokenize, echo=TRUE}
options(mc.cores=1)
unigrams <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
bigrams <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigrams <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

uniCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = unigrams))
biCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = bigrams))
triCorpus <- TermDocumentMatrix(corpus, control=list(tokenize = trigrams))

```

## Word frequency  
*Some words are more frequent than others - what are the distributions of word frequencies?*  

```{r freqWords, echo=TRUE}
freqTerms <- findFreqTerms(uniCorpus, lowfreq = 5)
freqTable <- rowSums(as.matrix(uniCorpus[freqTerms,]))
freqTable <- data.frame(word=names(freqTable), freq=freqTable)
freqTable <- freqTable[order(-freqTable$freq), ]
freqTable$rank <- 1:length(freqTable[,1])
kable(freqTable[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable$freq))
```

### Zipf's Law  

Via [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law): "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table."

Zipf's Law holds for all three of our corpora.  

```{r zipfPlot}
ggplot(freqTable, aes(log10(freq), log10(rank))) + geom_line(color="red") + labs(title="Zipf's Law") + theme(panel.background=element_blank())
```

## Bi- and tri-gram frequency  

*What are the frequencies of 2-grams and 3-grams in the dataset?*  

```{r ngramFreq}
freqTerms2 <- findFreqTerms(biCorpus, lowfreq = 5)
freqTable2 <- rowSums(as.matrix(biCorpus[freqTerms2,]))
freqTable2 <- data.frame(word=names(freqTable2), freq=freqTable2)
freqTable2 <- freqTable2[order(-freqTable2$freq), ]
freqTable2$rank <- 1:length(freqTable2[,1])
kable(freqTable2[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable2$freq))

freqTerms3 <- findFreqTerms(triCorpus, lowfreq = 5)
freqTable3 <- rowSums(as.matrix(triCorpus[freqTerms3,]))
freqTable3 <- data.frame(word=names(freqTable3), freq=freqTable3)
freqTable3 <- freqTable3[order(-freqTable3$freq), ]
freqTable3$rank <- 1:length(freqTable3[,1])
kable(freqTable3[1:10, c(3,1,2) ], row.names=FALSE)

hist(log10(freqTable3$freq))

```

## Language coverage  
*How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?*   

```{r}
allWordInstances <- sum(freqTable$freq)
freqTable$Sum <- cumsum(freqTable$freq)

fifty <- subset(freqTable, freqTable$Sum >  allWordInstances*0.5)
ninety <- subset(freqTable, freqTable$Sum >  allWordInstances*0.9)
```
To compute this, I calculated a cumulative sum of the token frequencies and stopped at the row that passed 50%/90% of total tokens.  

To cover 50% of all word instances, a dictionary of only **`r fifty[1,3]`** words are needed. To cover 90% of all word instances, a dictionary of **`r fifty[1,3]`** words are needed.


```{r}
# How do you evaluate how many of the words come from foreign languages? 

```

```{r}
# Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

```


```{r cleanCorpus}
# Remove unwanted characters (HTML tags, etc.)

# Word boundaires (white space, punctuation [consider Ph.D., isn't, e-mail])
#test <- twitter[1:10,]
#test <- paste(test, collapse=" ")
#test <- strsplit(test, "\\?+ |\\!+ |\\.+ ")

# Stemming (optional, most common is Porter Stemmer)

# Stopword removal (the, a, of, for, in; SMART is a common stopword list) - is this helpful for this ???

```

```{r trainPredictSimple}
# combine corpora
# separate into train and test sets
# get frequency distribution of two word ngrams


m <- Corpus(VectorSource(blog))
```
